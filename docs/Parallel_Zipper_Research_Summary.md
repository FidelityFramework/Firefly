# Parallel Zipper Research: Executive Summary

**Date**: January 27, 2026
**Status**: Research Complete, Design Proposal Ready
**Context**: Alex XParsec Remediation - Parallel Compilation Architecture

---

## Quick Reference: Key Findings

| Question | Answer | Evidence |
|----------|--------|----------|
| **Are nanopasses zipper traversals?** | YES (functional recursion with catamorphisms) | Nanopass framework `,[expr]` syntax |
| **Do nanopasses run in parallel?** | NO (sequential due to data dependencies) | Chez Scheme 50+ passes run sequentially |
| **Can Alex parallelize?** | YES (at function level, not pass level) | Pre-computed coeffects enable fan-out |
| **How does Triton parallelize?** | Function-level via MLIR threading | PassManager with context threading |
| **What's the right granularity?** | Function-level (natural F# boundary) | Proven in LLVM, matches SSA scope |
| **Expected speedup?** | 1.5-2× for programs with ≥10 functions | Typical multi-core benefit |

---

## The Hypothesis

> **"Each nanopass *is* a zipper traversal with associative hooks to re-accumulate into a final folded graph."**

### Verdict: ✅ YES, with Qualifications

**What's TRUE**:
- Nanopasses ARE zipper-based traversals (focus + context + recursion)
- They use catamorphisms (`,[expr]`) to recurse and bind results
- Each pass produces immutable output (functional purity)
- Composition is associative in the mathematical sense

**What's FALSE**:
- Traditional nanopass compilers do NOT run passes in parallel
- Passes have data dependencies (Pass N+1 needs Pass N's output)
- Framework provides NO parallelization hooks
- Production compilers (Chez Scheme, Racket) run passes sequentially

**The Nuance**:
- Parallelism exists at **MODULE/FUNCTION level**, not **PASS level**
- Different functions can be compiled concurrently (within same pass)
- Different modules can be compiled concurrently (embarrassingly parallel)
- This is what LLVM, MLIR, and Rust do (not pass-level parallelism)

---

## Research Summary by Source

### 1. Nanopass Framework (Scheme)

**What We Learned**:

Nanopasses use **functional zipper pattern** via catamorphisms:

```scheme
(define-pass remove-one-armed-if : Lsrc (e) -> L1 ()
  (Expr : Expr (e) -> Expr ()
    [(if ,[e0] ,[e1]) '(if ,e0 ,e1 (void))]  ; Catamorphism
    ...autogenerated clauses...))
```

**The `,[expr]` syntax**:
- Recurses into sub-expression
- Binds transformed result
- This IS the zipper pattern (focus on `expr`, recurse, bind result)

**Pass composition**:
```scheme
; Sequential pipeline
(pass3 (pass2 (pass1 input)))
```

**Parallelization**: NONE
- Framework has no parallel execution support
- Each pass runs to completion before next begins
- This is BY DESIGN (data dependencies)

**Key Insight**: The zipper pattern doesn't require imperative focus manipulation. It can be expressed through functional recursion.

---

### 2. Triton-CPU (MLIR Compiler)

**What We Learned**:

Triton uses **MLIR's PassManager** with context-level threading:

```python
pm = ir.pass_manager(mod.context)  # Context can enable threading
pm.addPass(InlinerPass())
pm.addPass(RewriteTensorPointerPass())
pm.run(mod)  # Blocking, handles parallelization internally
```

**Parallelization Scope**:
- ✅ **Function-level**: Different functions processed concurrently (within pass)
- ❌ **Pass-level**: Passes run sequentially (TTIR → TTGPU → LLIR)
- ✅ **Module-level**: Different modules can compile concurrently

**How it works**:
- MLIRContext has ThreadPool
- Operations have individual locks (thread-safe mutation)
- PassManager coordinates access
- Disabled in debug mode (crash reproduction, stack traces)

**Dependency tracking**:
```tablegen
def TritonGPUPipeline : Pass<...> {
  let dependentDialects = [
    "mlir::triton::gpu::TritonGPUDialect",
    "mlir::scf::SCFDialect"
  ];
}
```

Dependencies are **dialect requirements**, not explicit pass ordering.

**Key Insight**: Production MLIR compilers parallelize WITHIN passes (across functions), not ACROSS passes.

---

### 3. Chez Scheme (Production Compiler)

**What We Learned**:

Chez converted from **10 monolithic passes** to **50+ nanopasses**:

```
S-expression → Syntax → Lsrc → [50+ passes] → Machine code
```

**Performance Results**:
- 15-27% **faster code** than original compiler
- Within **2× compilation time** (despite 5× more passes)
- Proves nanopass viable for production

**Parallelization**:
- ❌ **NO pass-level parallelism** (data dependencies prevent it)
- ✅ **Module-level parallelism** (compile separate files concurrently)
- ✅ **Function-level parallelism** (backend code generation)
- ✅ **Incremental compilation** (only recompile changed modules)

**Why sequential?**
1. Data dependencies (Pass N+1 needs Pass N's complete output)
2. Whole-program analysis (e.g., cross-function inlining)
3. Cache locality (sequential processing improves cache hits)

**Key Insight**: Fine-grained passes enable better debugging/testing but enforce strict sequential execution.

---

## Alex's Unique Opportunity

### Why Alex Can Do What Nanopass Cannot

| Property | Traditional Nanopass | Alex |
|----------|---------------------|------|
| **Dependencies** | Computed DURING pass | Pre-computed (coeffects) |
| **Traversal** | Functional recursion | Zipper navigation |
| **Mutation** | Produces new IR | Immutable PSG (read-only) |
| **Witnesses** | Transform structure | Observe and return (codata) |
| **Accumulation** | Each pass = new IR | MLIR ops accumulate |

**The Critical Difference**: Pre-computed coeffects

```fsharp
// Nanopass: Computes dependencies DURING traversal
let transform node =
    let deps = computeDependencies node  // Happens NOW
    buildOutput deps

// Alex: Dependencies ALREADY KNOWN
let witness ctx node =
    let deps = ctx.Coeffects.SSA.getDependencies node.Id  // Pre-computed
    observeAndReturn deps
```

**Implication**: If dependencies are already known, we can:
1. Build dependency graph **before traversal**
2. Identify independent functions
3. Create zippers for each function
4. Witness in parallel
5. Merge results associatively

---

## The Recommended Architecture

### Function-Level Fan-Out/Fold-In

```
Entry Point
    ↓
Build Dependency Graph (from SSA coeffect)
    ↓
Topological Sort → Batches
    ↓
For each batch:
    ┌──────────┬──────────┬──────────┬──────────┐
  Function A  Function B  Function C  Function D
  Zipper₁     Zipper₂     Zipper₃     Zipper₄    (Array.Parallel.map)
  Acc₁        Acc₂        Acc₃        Acc₄
    │           │           │           │
    └───────────┴───────────┴───────────┘
                  ↓
      Associative Merge (Array.fold)
                  ↓
        Global Accumulator
    ↓
Next batch...
```

### Four Key Decisions

#### 1. Granularity: **Function-Level**

**Why?**
- Natural F# program boundary
- SSA dependencies mostly intra-function
- MLIR function definitions order-independent
- Proven in LLVM/MLIR/Rust

**Alternatives considered**:
- Module-level (too coarse - little parallelism)
- Expression-level (too fine - overhead > benefit)

#### 2. Dependency Tracking: **Reuse SSA Coeffect**

**Why?**
- Already captures def-use edges
- NO new analysis needed
- Correct by construction (PSGElaboration computed it)

**Implementation**:
```fsharp
let getFunctionDependencies (func: SemanticNode) (ssa: SSAAssignment) : Set<NodeId> =
    // Collect all nodes this function uses
    let descendantscompletedDescendants func
    // Find dependencies outside this function
    descendants
    |> List.collect (fun n -> Set.toList (ssa.getDependencies n.Id))
    |> List.filter (fun depId -> not (isDescendantOf depId func))
    |> Set.ofList
```

#### 3. Merge Strategy: **Batch-Wise**

**Why?**
- Simple to implement
- Easy to reason about
- Clear synchronization points
- Sufficient for typical programs

**Implementation**:
```fsharp
for batch in batches do
    let batchResults =
        batch
        |> Array.ofList
        |> Array.Parallel.map compileUnit  // Parallel
        |> Array.fold mergeAccumulators (MLIRAccumulator.empty())  // Sequential merge

    globalAcc <- mergeAccumulators globalAcc batchResults
```

**Alternatives considered**:
- Tree-reduction (logarithmic depth - defer until profiling shows merge bottleneck)
- Streaming (complex synchronization - defer)

#### 4. Heuristics: **Only Parallelize if Beneficial**

**Cost Model**:
```fsharp
let shouldParallelize (batch: CompilationUnit list) : bool =
    let count = List.length batch
    let avgComplexity = batch |> List.averageBy (fun u -> countNodes u)

    count >= 4 &&                      // At least 4 functions
    avgComplexity >= 50 &&             // Each non-trivial (50+ nodes)
    count * avgComplexity >= 500       // Total work substantial
```

**Why?**
- Small programs: overhead of zipper creation + merge > benefit
- Debugging: sequential execution easier to trace
- Tunability: parameters can be adjusted via profiling

---

## Associative Operations

### What CAN Be Parallelized (Order-Independent)

✅ **Function definitions**:
```mlir
func.func @f1() { ... }
func.func @f2() { ... }
// Order doesn't matter in MLIR module
```

✅ **Global constants**:
```mlir
%c1 = arith.constant 42 : i32
%c2 = arith.constant 17 : i32
// Order doesn't matter
```

✅ **Type definitions**:
```mlir
!ty1 = type { i32, i64 }
!ty2 = type { ptr, i32 }
// Order doesn't matter
```

### What CANNOT Be Parallelized (Order-Dependent)

❌ **Basic blocks within function**:
```mlir
^bb0:
  %0 = ...
  br ^bb1(%0)  // Must come before bb1
^bb1(%arg: i32):
  ...
```
Dominance requires specific ordering.

❌ **Operations within block**:
```mlir
%0 = arith.addi %a, %b
%1 = arith.muli %0, %c  // Depends on %0
```
SSA requires def before use.

### The Solution

**Parallelize at function boundaries, sequential within functions**:

```fsharp
// Different functions: PARALLEL
async {
    let! func1MLIR = witnessFunction ctx func1Id
    let! func2MLIR = witnessFunction ctx func2Id
    return merge func1MLIR func2MLIR  // Order-independent
}

// Within function: SEQUENTIAL
let witnessFunctionBody ctx funcNode =
    // Basic blocks witnessed in dominance order
    // Operations witnessed in SSA order
```

---

## Implementation Roadmap

### Milestone 1: Sequential Baseline (1-2 days)

**Goal**: Establish correct function-level compilation

**Tasks**:
1. Implement `buildUnitDependencyGraph` (from SSA coeffect)
2. Implement `toposort` (batches of independent functions)
3. Implement sequential batch execution
4. Verify all HelloWorld samples produce identical output

**Validation**:
```bash
cd tests/regression
dotnet fsi Runner.fsx -- --parallel
# All tests PASS, output identical to current implementation
```

**Deliverables**:
- `DependencyGraph.fs` - Unit dependency analysis
- `TopoSort.fs` - Batch identification
- `SequentialCompiler.fs` - Baseline implementation

---

### Milestone 2: Parallel Execution (2-3 days)

**Goal**: Enable function-level parallelism

**Tasks**:
1. Replace `List.map` with `Array.Parallel.map` for batch processing
2. Implement `mergeAccumulators` (associative)
3. Add parallelization heuristics (`shouldParallelize`)
4. Add `ParallelizationConfig` (tunable parameters)
5. Test: parallel output = sequential output

**Validation**:
```fsharp
[<Test>]
let ``parallel compilation produces same output as sequential`` () =
    let seqOutput = compileSequential graph coeffects
    let parOutput = compileParallel graph coeffects

    Assert.AreEqual(seqOutput, parOutput)
```

**Deliverables**:
- `ParallelCompiler.fs` - Parallel implementation
- `ParallelizationConfig.fs` - Heuristics and tuning
- Unit tests for merge associativity

---

### Milestone 3: Profiling & Optimization (2-3 days)

**Goal**: Measure and optimize performance

**Tasks**:
1. Add timing instrumentation (per-function, per-batch, total)
2. Profile HelloWorld samples (baseline: small, few functions)
3. Profile larger samples (TimeLoop: loops + mutable state)
4. Create multi-function test program (10-20 functions)
5. Measure speedup: `time(parallel) / time(sequential)`
6. Tune heuristics (MinBatchSize, MinUnitComplexity)
7. If merge is bottleneck: implement tree-reduction

**Validation**:
```
Sample              Functions  Sequential  Parallel  Speedup
01_HelloWorldDirect      1       100ms      110ms    0.9×   (overhead > benefit)
TimeLoop                 3       200ms      190ms    1.05×  (marginal)
MultiFunction           10       500ms      300ms    1.67×  (good)
LargeProgram            30      1500ms      800ms    1.88×  (excellent)
```

**Deliverables**:
- Performance report
- Tuned heuristics
- Optimized merge (if needed)
- Documentation on when parallelization helps

---

### Milestone 4: Testing & Documentation (1-2 days)

**Goal**: Ensure correctness and preserve knowledge

**Tasks**:
1. Create unit tests for `buildUnitDependencyGraph`
2. Create unit tests for `toposort` (handles cycles, correct batching)
3. Create property tests for `mergeAccumulators` (associativity, commutativity)
4. Update documentation (`docs/Parallel_Zipper_Architecture.md`)
5. Update Serena memories
6. Code review

**Validation**:
- 100% regression test pass rate
- Property tests pass with randomized inputs
- All edge cases covered (cycles, single function, empty graph)

**Deliverables**:
- Complete test suite
- Updated documentation
- Team review sign-off

---

## Expected Benefits

### Performance

**Speedup Predictions** (based on LLVM/MLIR research):

| Program Size | Functions | Expected Speedup | Reason |
|--------------|-----------|------------------|--------|
| Tiny (HelloWorld) | 1-2 | 0.9-1.0× | Overhead > benefit |
| Small (TimeLoop) | 3-5 | 1.0-1.2× | Marginal benefit |
| Medium | 10-15 | 1.5-1.8× | Good parallelism |
| Large | 20+ | 1.8-2.5× | Excellent parallelism |

**Factors**:
- CPU cores available (4-16 typical)
- Function complexity (small functions = less benefit)
- Dependency graph structure (deep chains = less parallelism)

### Code Quality

**NO regressions expected**:
- Witnesses unchanged (same observation logic)
- Merge is associative (order-independent)
- Sequential baseline validates correctness
- Property tests ensure invariants

---

## Open Questions & Future Work

### 1. Within-Function Parallelism?

**Question**: Can we parallelize expression-level (within function)?

**Example**:
```fsharp
let x = expensive1()
let y = expensive2()  // No dependency on x
let z = x + y
```

**Challenge**: Requires finer dependency analysis (intra-function)

**Decision**: **DEFER** to future work. Function-level sufficient for initial implementation.

---

### 2. Cross-Module Optimization?

**Question**: Can we do whole-program optimization across parallel units?

**Example**: Inlining function from Module A into Module B

**Challenge**: Requires sequential knowledge of both modules

**Solution**: Run optimization passes **sequentially AFTER** parallel compilation

**Decision**: Parallelize code generation, optimize sequentially after merge.

---

### 3. Async/Streaming Merge?

**Question**: Can we start merging while units still compiling?

**Benefit**: Reduce latency (don't wait for entire batch)

**Challenge**: Requires careful synchronization (more complexity)

**Decision**: **DEFER**. Batch-wise is simpler and sufficient.

---

### 4. Pass-Level Parallelism?

**Question**: Can different PASSES run in parallel (not just units within pass)?

**Example**: If Pass A and Pass B are independent, run concurrently?

**Challenge**: Most passes have data dependencies (A's output → B's input)

**Exception**: Whole-program analysis passes might be independent

**Decision**: **DEFER**. Not applicable to current Alex architecture (witnesses, not passes).

---

## Key Takeaways

### For Implementation

1. ✅ **Function-level granularity** is proven (LLVM, MLIR, Rust)
2. ✅ **SSA coeffect** already has dependency info (reuse it)
3. ✅ **Batch-wise merge** is simple and sufficient
4. ✅ **Heuristics** prevent overhead on small programs
5. ✅ **Sequential baseline** validates correctness

### For Architecture

1. ✅ **Immutable PSG** enables safe parallel reads
2. ✅ **Codata witnesses** are naturally parallelizable (pure observers)
3. ✅ **Pre-computed coeffects** enable dependency analysis before traversal
4. ✅ **Associative accumulation** enables order-independent merge
5. ✅ **This architecture is UNIQUE** - nanopass doesn't have these properties

### For Understanding

1. ✅ **Nanopasses ARE zippers** (functional recursive pattern)
2. ✅ **But passes run sequentially** (data dependencies)
3. ✅ **Parallelism is within-pass** (across functions), not across-passes
4. ✅ **Alex can do better** because dependencies are pre-computed
5. ✅ **Expected speedup: 1.5-2×** for typical programs

---

## References

### Documentation Created

- `/home/hhh/repos/Firefly/docs/Parallel_Zipper_Architecture.md` - Full architectural analysis
- `/home/hhh/repos/Firefly/docs/Parallel_Zipper_Design_Synthesis.md` - Complete design proposal (54 pages)
- `/home/hhh/repos/Firefly/docs/Next_Context_Handoff_Parallel_Zippers.md` - Research plan (executed)
- This document - Executive summary

### Serena Memories

- `parallel_zipper_fan_out_fold_in` - Core findings and recommendations
- `alex_parallel_zipper_design_status_jan27` - Status checkpoint

### Research Agents

- **a8689dd**: Nanopass framework analysis (VERY THOROUGH)
- **a211d88**: Triton-CPU PassManager research (VERY THOROUGH)
- **af2a827**: Chez Scheme compiler research (general-purpose with web search)

### External Resources

- `~/repos/nanopass-framework-scheme/` - Nanopass implementation
- `~/triton-cpu/` - Production MLIR compiler
- Papers: Sarkar/Waddell/Dybvig (ICFP 2004, ICFP 2013), Keep dissertation (2013)

---

## Next Steps

### Immediate (After Current XParsec Remediation)

1. ✅ Complete witness pattern fixes (ArithWitness, LazyWitness destructuring)
2. ✅ Finish XParsec remediation (get all witnesses to ~20 lines)
3. ⬜ Implement Milestone 1 (sequential baseline)
4. ⬜ Implement Milestone 2 (parallel execution)
5. ⬜ Implement Milestone 3 (profiling & optimization)

### Future (After Parallel Architecture Complete)

- Explore within-function parallelism (expression-level)
- Whole-program optimization passes
- Incremental compilation (only recompile changed functions)
- Streaming merge (reduce latency)

---

**Status**: Research phase complete. Design proposal ready. Ready to implement after XParsec remediation.

**Expected Timeline**: 6-10 days for complete implementation (4 milestones)

**Expected Outcome**: 1.5-2× speedup for typical multi-function programs on multi-core systems

---

**The standing art composes up. Use it.**
