# Parallel Zipper Architecture: Design Synthesis

**Date**: January 27, 2026
**Status**: Research Complete, Design Proposal
**Context**: Alex XParsec Remediation - Parallel Zipper Discovery

---

## Executive Summary

After comprehensive research into nanopass framework, Triton-CPU, and Chez Scheme, I can now answer the hypothesis:

> **"Each nanopass *is* a zipper traversal with associative hooks to re-accumulate into a final folded graph."**

**Answer: YES, with critical qualifications.**

Nanopasses ARE zipper-based functional traversals, BUT:
- They use **functional recursion with catamorphisms**, not imperative focus manipulation
- Passes run **sequentially** due to data dependencies (output of pass N → input of pass N+1)
- **NO pass-level parallelism** exists in production nanopass compilers
- Parallelization happens at **module/function granularity**, not pass granularity

**However**, Alex has architectural properties that enable parallelism where nanopass doesn't:

1. **Pre-computed coeffects** - Dependencies known before traversal
2. **Immutable PSG** - Multiple readers can access simultaneously
3. **Codata witnesses** - Pure observers that return, don't mutate
4. **Associative accumulation** - MLIR operations can merge order-independently

**The opportunity**: Fan-out/fold-in at **function or module level**, not pass level.

---

## Research Findings Summary

### 1. Nanopass Framework (Scheme)

**Architecture**:
```scheme
(define-pass remove-one-armed-if : Lsrc (e) -> L1 ()
  (Expr : Expr (e) -> Expr ()
    [(if ,[e0] ,[e1]) '(if ,e0 ,e1 (void))]  ; Catamorphism
    ...autogenerated clauses...))
```

**Key Properties**:
- **Zipper pattern**: Focus (current node), context (parent via record fields), catamorphism recursion
- **Functional purity**: Each pass produces NEW IR (immutable)
- **Pattern matching**: Template-based construction (`'(if ,e0 ,e1 ...)`)
- **Autogeneration**: Framework generates missing clauses and traversal boilerplate
- **Sequential execution**: Passes chained `(pass3 (pass2 (pass1 input)))`

**Catamorphism Advantage**:
```scheme
; The `,[expr]` syntax BOTH recurses AND binds result
[(lambda (,x* ...) ,[body*] ... ,[body])
 '(lambda (,x* ...) ,body* ... ,body)]
```

**NO Parallelism**: Framework is inherently sequential. Each pass must complete before next begins.

---

### 2. Triton-CPU (MLIR)

**PassManager Architecture**:
```cpp
// From ir.cc
pm = ir.pass_manager(mod.context)
pm.addPass(...)
pm.run(mod)  // Blocking, runs all passes
```

**Parallelization Strategy**:
- **Context-level threading**: MLIRContext can enable multithreading
- **Function-level granularity**: Passes run on different functions concurrently
- **In-place mutation**: No copying, structural sharing with operation locks
- **Sequential stages**: TTIR → TTGPU → LLIR happens sequentially
- **Disabled when debugging**: Single-threaded for crash reproduction/stack traces

**Key Finding**: Parallelism is **within a pass** (across functions), NOT **across passes**.

**Dependency Model**:
```tablegen
def TritonGPUPipeline : Pass<"tritongpu-pipeline", "mlir::ModuleOp"> {
  let dependentDialects = [
    "mlir::triton::gpu::TritonGPUDialect",
    "mlir::scf::SCFDialect"
  ];
}
```

Dependencies are **dialect requirements**, not explicit pass ordering.

---

### 3. Chez Scheme (Production Nanopass Compiler)

**Pipeline Structure**:
- **50+ nanopasses** (replaced 5 monolithic passes)
- **Sequential execution**: Each pass transforms entire program before next
- **Performance**: 15-27% faster code than original compiler
- **Compilation time**: Within 2× despite 5× more passes

**Parallelization**:
- ❌ **NO pass-level parallelism**
- ✅ **Module-level parallelism** (compile separate files concurrently)
- ✅ **Function-level parallelism** (LLVM can run function passes in parallel)
- ✅ **Backend parallelism** (code generation units processed independently)

**Why Sequential?**
1. **Data dependencies**: Output of pass N is input to pass N+1
2. **Whole-program analysis**: Many optimizations require complete program info
3. **Cache locality**: Sequential function-by-function processing improves cache hits

---

## The Critical Insight: Where Parallelism Actually Exists

### What Traditional Nanopass DOESN'T Parallelize

```
Pass 1 → Pass 2 → Pass 3 → Pass 4 → Pass 5
(Each pass must complete before next begins)
```

**Why**: Data dependencies. Pass 2 needs the transformed output from Pass 1.

### What Production Compilers DO Parallelize

**1. Module-Level Parallelism (Embarrassingly Parallel)**:
```
Module A → [Pass 1...Pass N] → Output A
Module B → [Pass 1...Pass N] → Output B  } Parallel
Module C → [Pass 1...Pass N] → Output C
```

**2. Function-Level Parallelism (Within Pass)**:
```
Pass X operating on:
  Function 1 }
  Function 2 } Parallel (if independent)
  Function 3 }
```

**3. Backend Parallelism (Code Generation)**:
```
Function 1 → LLVM CodeGen }
Function 2 → LLVM CodeGen } Parallel
Function 3 → LLVM CodeGen }
```

---

## Alex's Unique Opportunity

### Why Alex is Different from Traditional Nanopass

| Aspect | Traditional Nanopass | Alex |
|--------|---------------------|------|
| **Dependencies** | Computed during pass | Pre-computed (coeffects) |
| **Traversal** | Catamorphism recursion | Zipper navigation |
| **Mutation** | Immutable → new IR | Immutable PSG (read-only) |
| **Witnesses** | Transform structure | Observe and return (codata) |
| **Accumulation** | N/A (produces new IR) | MLIR ops accumulate |

### The Key Difference: Pre-Computed Dependencies

**Nanopass**:
```scheme
; Computes captures DURING traversal
[(lambda (,x* ...) ,[body])
 (let ([free-vars (compute-free-vars body)])  ; COMPUTED HERE
   '(closure ,x* ,free-vars ,body))]
```

**Alex**:
```fsharp
// Captures ALREADY KNOWN from coeffects
let captures = ctx.Coeffects.CaptureAnalysis.getCapturesFor lambdaId
// Witness just observes
```

**Implication**: If dependencies are **already known**, we can:
1. Build dependency graph BEFORE traversal
2. Identify independent subtrees
3. Fan out zippers to independent subtrees
4. Fold in results associatively

---

## Design Proposal: Function-Level Fan-Out/Fold-In

### Architecture Overview

```
Entry Point
    ↓
Build Dependency Graph (from SSA coeffect)
    ↓
Topological Sort → Identify Batches
    ↓
    ┌─────────────┬─────────────┬─────────────┐
    ↓             ↓             ↓             ↓
Function A    Function B    Function C    Function D
Zipper₁       Zipper₂       Zipper₃       Zipper₄
Witness       Witness       Witness       Witness
Accumulator₁  Accumulator₂  Accumulator₃  Accumulator₄
    │             │             │             │
    └─────────────┴─────────────┴─────────────┘
                  ↓
        Associative Merge (fold-in)
                  ↓
         Unified MLIR Module
```

### Phase 1: Dependency Analysis

**Input**: SemanticGraph with coeffects

**Process**:
```fsharp
type CompilationUnit = {
    RootNode: NodeId           // Entry point (function/module)
    Dependencies: Set<NodeId>  // What this unit needs
    Provides: Set<NodeId>      // What this unit produces
}

let buildUnitDependencyGraph (graph: SemanticGraph) (coeffects: TransferCoeffects) : CompilationUnit list =
    // For each function in graph:
    graph.Functions
    |> List.map (fun funcNode ->
        let deps = SSAAssignment.getDependencies funcNode.Id coeffects.SSA
        let provides = SSAAssignment.getProvidedBindings funcNode.Id coeffects.SSA
        {
            RootNode = funcNode.Id
            Dependencies = deps
            Provides = provides
        })
```

**Output**: List of compilation units with explicit dependencies

### Phase 2: Topological Sort

**Input**: Compilation units with dependencies

**Process**:
```fsharp
let toposort (units: CompilationUnit list) : CompilationUnit list list =
    // Group into batches where units in same batch have no inter-dependencies
    // Batch 0: Units with no dependencies
    // Batch 1: Units depending only on Batch 0
    // Batch 2: Units depending on Batch 0 or 1
    // etc.

    let rec buildBatches (remaining: CompilationUnit list) (completed: Set<NodeId>) (batches: CompilationUnit list list) =
        match remaining with
        | [] -> List.rev batches
        | _ ->
            let ready = remaining |> List.filter (fun u ->
                u.Dependencies |> Set.forall (fun dep -> Set.contains dep completed))

            if List.isEmpty ready then
                failwith "Circular dependency detected"

            let newCompleted = ready |> List.fold (fun acc u -> Set.union acc u.Provides) completed
            let newRemaining = remaining |> List.filter (fun u -> not (List.contains u ready))

            buildBatches newRemaining newCompleted (ready :: batches)

    buildBatches units Set.empty []
```

**Output**: List of batches, where each batch contains independent units

### Phase 3: Parallel Execution

**Input**: Batches of independent units

**Process**:
```fsharp
let parallelCompile (graph: SemanticGraph) (coeffects: TransferCoeffects) (batches: CompilationUnit list list) : MLIRAccumulator =

    let globalAcc = MLIRAccumulator.empty()

    for batch in batches do
        // Units in this batch are independent - run in parallel
        let batchResults =
            batch
            |> Array.ofList
            |> Array.Parallel.map (fun unit ->
                async {
                    // Create zipper for this unit's entry point
                    let zipper = PSGZipper.create graph unit.RootNode |> Option.get

                    // Create fresh accumulator for this unit
                    let unitAcc = MLIRAccumulator.empty()

                    // Create context
                    let ctx = {
                        Graph = graph
                        Coeffects = coeffects
                        Accumulator = unitAcc
                        Zipper = zipper
                    }

                    // Visit this unit's subgraph
                    let output = visitNode ctx unit.RootNode unitAcc

                    return unitAcc
                }
                |> Async.RunSynchronously)

        // Fold-in: Merge batch results associatively
        let batchAcc = batchResults |> Array.fold mergeAccumulators (MLIRAccumulator.empty())

        // Merge into global accumulator
        mergeAccumulators globalAcc batchAcc |> ignore

    globalAcc
```

**Key Properties**:
- **Within batch**: Parallel execution (Array.Parallel.map)
- **Across batches**: Sequential (for loop) to respect dependencies
- **Each unit**: Independent zipper + accumulator
- **Merge**: Associative fold

### Phase 4: Associative Merge

**Requirements for Correctness**:

```fsharp
let mergeAccumulators (acc1: MLIRAccumulator) (acc2: MLIRAccumulator) : MLIRAccumulator =
    // MUST be associative: merge (merge acc1 acc2) acc3 = merge acc1 (merge acc2 acc3)

    {
        TopLevelOps = acc1.TopLevelOps @ acc2.TopLevelOps  // Order-independent for functions
        Visited = Set.union acc1.Visited acc2.Visited
        Bindings = Map.union acc1.Bindings acc2.Bindings  // No conflicts (independent units)
    }
```

**What Operations Are Associative?**

✅ **Safe for parallel merge**:
- Function definitions (order-independent in MLIR module)
- Global constants
- Type definitions
- External declarations

❌ **Require ordering**:
- Basic blocks within function (dominance)
- Operations within block (SSA def before use)

**Solution**: Parallelize at **function level**. Within-function operations remain sequential.

---

## Granularity Decisions

### Option 1: Function-Level Parallelism (RECOMMENDED)

**Granularity**: Each function is a compilation unit

**Pros**:
- Natural boundary in F# programs
- SSA dependencies mostly intra-function
- MLIR function definitions are order-independent
- Easy to implement (clear unit boundaries)
- Proven in LLVM/MLIR

**Cons**:
- Coarser than expression-level (less parallelism)
- Requires handling cross-function calls (via dependency graph)

**Implementation**:
```fsharp
let units = graph.Functions |> List.map (fun f -> { RootNode = f.Id; ... })
```

### Option 2: Module-Level Parallelism

**Granularity**: Each F# module is a compilation unit

**Pros**:
- Coarsest granularity (simplest)
- Modules are independent by design
- Embarrassingly parallel

**Cons**:
- Too coarse for most programs
- Little benefit for single-module programs

### Option 3: Expression-Level Parallelism

**Granularity**: Each independent expression tree is a unit

**Pros**:
- Finest granularity (maximum parallelism)

**Cons**:
- Overhead of zipper creation
- Complex dependency analysis
- Merge overhead exceeds benefit for small expressions

**Decision**: Start with **Function-Level**, add Module-Level as optimization.

---

## Dependency Tracking Strategy

### Reuse SSA Coeffect

**Already Computed**:
```fsharp
type SSAAssignment = {
    Assignments: Map<NodeId, SSA>
    Dependencies: Map<NodeId, Set<NodeId>>  // Node → nodes it depends on
}
```

**Usage**:
```fsharp
let getDependencies (nodeId: NodeId) (ssa: SSAAssignment) : Set<NodeId> =
    match Map.tryFind nodeId ssa.Dependencies with
    | Some deps -> deps
    | None -> Set.empty
```

**For function-level parallelism**:
```fsharp
let getFunctionDependencies (funcNode: SemanticNode) (ssa: SSAAssignment) : Set<NodeId> =
    // Collect all nodes this function references
    let allNodes = collectDescendants funcNode

    // Find dependencies outside this function
    allNodes
    |> List.collect (fun n -> Set.toList (getDependencies n.Id ssa))
    |> List.filter (fun depId -> not (List.exists (fun n -> n.Id = depId) allNodes))
    |> Set.ofList
```

**NO new analysis needed** - reuse existing SSA coeffect.

---

## Merge Strategy

### Batch-Wise Merge (RECOMMENDED)

**Process**:
1. Execute all units in batch concurrently
2. Wait for all to complete
3. Merge accumulators associatively
4. Proceed to next batch

**Advantages**:
- Simple to implement
- Easy to reason about
- Clear synchronization points
- Matches topological sort structure

**Implementation**:
```fsharp
for batch in batches do
    let results = batch |> Array.Parallel.map compileUnit
    let batchAcc = results |> Array.fold mergeAccumulators (MLIRAccumulator.empty())
    globalAcc <- mergeAccumulators globalAcc batchAcc
```

### Alternative: Tree Reduction

**Process**: Merge accumulators in binary tree

```
Acc₁  Acc₂  Acc₃  Acc₄
  \    /      \    /
   Acc₁₂       Acc₃₄
       \       /
        Acc₁₂₃₄
```

**Advantages**: Logarithmic depth (faster for large batches)

**Disadvantages**: More complex, marginal benefit for typical programs

**Decision**: Use **Batch-Wise** initially, optimize to tree reduction if profiling shows merge is bottleneck.

---

## Heuristics: When to Parallelize

### Cost Model

**Only parallelize if benefits exceed overhead**:

```fsharp
let shouldParallelize (batch: CompilationUnit list) : bool =
    let unitCount = List.length batch
    let avgComplexity = batch |> List.averageBy estimateComplexity

    // Heuristics (tune via profiling):
    unitCount >= 4 &&                    // At least 4 units
    avgComplexity >= 50 &&               // Each unit non-trivial (e.g., 50+ nodes)
    unitCount * avgComplexity >= 500     // Total work substantial
```

**Complexity Estimation**:
```fsharp
let estimateComplexity (unit: CompilationUnit) : int =
    let node = SemanticGraph.getNode unit.RootNode graph
    countDescendants node  // Approximation: node count
```

**Fallback to Sequential**:
```fsharp
if shouldParallelize batch then
    Array.Parallel.map compileUnit (Array.ofList batch)
else
    List.map compileUnit batch |> Array.ofList
```

### Tunable Parameters

```fsharp
type ParallelizationConfig = {
    MinBatchSize: int           // Default: 4
    MinUnitComplexity: int      // Default: 50
    MinTotalWork: int           // Default: 500
    EnableParallelization: bool // Default: true (disable for debugging)
}
```

**Debugging Mode**: Disable parallelization entirely
```fsharp
let config = if isDebugMode then
    { ParallelizationConfig.default with EnableParallelization = false }
else
    ParallelizationConfig.default
```

---

## Implementation Roadmap

### Milestone 1: Sequential Baseline (1-2 days)

**Goal**: Establish correct sequential function-level compilation

**Tasks**:
1. ✅ Zipper in WitnessContext (DONE)
2. ✅ MLIRTransfer shadows ctx (DONE)
3. ✅ Witnesses use ctx.Zipper (PARTIALLY DONE - LazyWitness, ArithWitness patterns need fixing)
4. Implement `buildUnitDependencyGraph`
5. Implement `toposort`
6. Implement sequential batch execution
7. Verify HelloWorld samples compile correctly

**Validation**: All samples work identically to current implementation

### Milestone 2: Parallel Execution (2-3 days)

**Goal**: Enable parallel function-level compilation

**Tasks**:
1. Implement `parallelCompile` with Array.Parallel.map
2. Implement `mergeAccumulators` (associative)
3. Add parallelization heuristics
4. Add ParallelizationConfig
5. Test with multi-function samples

**Validation**: Parallel compilation produces identical output to sequential

### Milestone 3: Profiling & Optimization (2-3 days)

**Goal**: Measure and optimize parallel performance

**Tasks**:
1. Add timing instrumentation
2. Profile HelloWorld samples
3. Profile larger samples (TimeLoop, etc.)
4. Tune heuristics (MinBatchSize, MinUnitComplexity)
5. Optimize merge strategy if bottleneck
6. Document performance characteristics

**Validation**: Speedup ≥ 1.5× for programs with ≥10 functions

### Milestone 4: Testing & Documentation (1-2 days)

**Goal**: Ensure correctness and preserve knowledge

**Tasks**:
1. Create test suite for dependency graph building
2. Create test suite for topological sort
3. Create test suite for merge associativity
4. Update documentation
5. Update Serena memories
6. Code review

**Validation**: 100% regression test pass rate

---

## Testing Strategy

### Unit Tests

```fsharp
[<Test>]
let ``buildUnitDependencyGraph handles simple case`` () =
    // Graph: main calls f, f calls g
    let units = buildUnitDependencyGraph simpleGraph coeffects

    let mainUnit = units |> List.find (fun u -> u.RootNode = mainId)
    let fUnit = units |> List.find (fun u -> u.RootNode = fId)
    let gUnit = units |> List.find (fun u -> u.RootNode = gId)

    Assert.Contains(fId, mainUnit.Dependencies)
    Assert.Contains(gId, fUnit.Dependencies)
    Assert.IsEmpty(gUnit.Dependencies)

[<Test>]
let ``toposort produces correct batches`` () =
    let batches = toposort units

    // Batch 0 should have no dependencies (g)
    Assert.AreEqual(1, batches.[0].Length)
    Assert.Contains(gId, batches.[0] |> List.map (fun u -> u.RootNode))

    // Batch 1 should depend only on Batch 0 (f)
    Assert.AreEqual(1, batches.[1].Length)

    // Batch 2 should depend on Batch 1 (main)
    Assert.AreEqual(1, batches.[2].Length)

[<Test>]
let ``mergeAccumulators is associative`` () =
    let acc1 = createAcc [op1; op2]
    let acc2 = createAcc [op3; op4]
    let acc3 = createAcc [op5; op6]

    let result1 = mergeAccumulators (mergeAccumulators acc1 acc2) acc3
    let result2 = mergeAccumulators acc1 (mergeAccumulators acc2 acc3)

    Assert.AreEqual(result1.TopLevelOps, result2.TopLevelOps)  // Order may differ, but content same
```

### Integration Tests

```fsharp
[<Test>]
let ``parallel compilation produces same output as sequential`` () =
    let seqOutput = compileSequential graph coeffects
    let parOutput = compileParallel graph coeffects

    Assert.AreEqual(seqOutput, parOutput)

[<Test>]
let ``parallel compilation handles circular dependencies`` () =
    // Graph with mutual recursion
    Assert.Throws<exn>(fun () -> compileParallel circularGraph coeffects)
```

### Regression Tests

Run all existing HelloWorld samples:
```bash
cd tests/regression
dotnet fsi Runner.fsx -- --parallel --verbose
```

All must pass with identical output.

---

## Open Questions & Future Work

### Within-Pass Parallelism?

**Question**: Can we parallelize WITHIN a function (expression-level)?

**Example**: Independent let bindings
```fsharp
let x = expensive1()
let y = expensive2()  // No dependency on x
let z = x + y
```

**Challenge**: Requires finer dependency analysis

**Decision**: DEFER to future work. Function-level parallelism is sufficient for initial implementation.

### Cross-Module Optimization?

**Question**: Can we do whole-program optimization across parallel units?

**Challenge**: Inlining across functions requires sequential knowledge

**Solution**: Run optimization passes sequentially AFTER parallel compilation

### Async/Streaming Merge?

**Question**: Can we start merging while units still compiling?

**Benefit**: Reduce latency (don't wait for entire batch)

**Challenge**: Requires careful synchronization, more complex

**Decision**: DEFER. Batch-wise is simpler and sufficient.

---

## Summary

**The Hypothesis Validated**:
> "Each nanopass *is* a zipper traversal with associative hooks to re-accumulate into a final folded graph."

**YES**: Nanopasses are zipper-based functional traversals with catamorphisms.

**BUT**: Traditional nanopass compilers don't parallelize passes (sequential pipeline).

**ALEX'S OPPORTUNITY**: Pre-computed coeffects enable function-level fan-out/fold-in parallelism.

**RECOMMENDED APPROACH**:
1. **Function-level granularity** (each function = compilation unit)
2. **Dependency graph from SSA coeffect** (no new analysis)
3. **Batch-wise merge** (simple, correct)
4. **Heuristics-based** (only parallelize when beneficial)

**EXPECTED BENEFIT**: 1.5-2× speedup for programs with ≥10 functions on multi-core systems.

**NEXT STEPS**:
1. Fix witness patterns (ArithWitness, LazyWitness)
2. Implement sequential baseline
3. Enable parallel execution
4. Profile and optimize
5. Document and test

---

**References**:
- Nanopass Framework research (agent a8689dd)
- Triton-CPU research (agent a211d88)
- Chez Scheme research (agent af2a827)
- `/home/hhh/repos/Firefly/docs/Parallel_Zipper_Architecture.md`
- `/home/hhh/repos/Firefly/docs/Next_Context_Handoff_Parallel_Zippers.md`
